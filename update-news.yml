name: Update Yemeni News
on:
  schedule:
    - cron: '*/5 * * * *'  # Runs every 5 minutes
  workflow_dispatch:       # Allows manual triggers
  push:
    branches: [ main ]     # Run when code changes

permissions:
  contents: write  # Needed to push changes

jobs:
  update-news:
    runs-on: ubuntu-latest
    timeout-minutes: 5     # Prevent hanging
    
    steps:
      # Step 1: Checkout code
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 0   # Get full git history
      
      # Step 2: Set up Python
      - name: Set up Python 3.10
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      # Step 3: Install dependencies
      - name: Install Python packages
        run: |
          python -m pip install --upgrade pip
          pip install feedparser beautifulsoup4 requests
      
      # Step 4: Create data directory if it doesn't exist
      - name: Create data directory
        run: mkdir -p data
      
      # Step 5: Run scraper
      - name: Scrape Yemeni news
        run: |
          echo '
          import feedparser
          import json
          from datetime import datetime
          from bs4 import BeautifulSoup
          import requests
          
          FEEDS = [
              "https://www.almashhad-alyemeni.com/feed",
              "https://yemennownews.com/feed",
              "https://www.24-post.com/feed"
          ]
          
          def get_feed(url):
              try:
                  headers = {"User-Agent": "Mozilla/5.0"}
                  response = requests.get(url, headers=headers, timeout=10)
                  return feedparser.parse(response.content)
              except Exception as e:
                  print(f"Error fetching {url}: {str(e)}")
                  return None
                  
          articles = []
          for feed_url in FEEDS:
              feed = get_feed(feed_url)
              if feed and feed.entries:
                  for entry in feed.entries[:30]:
                      try:
                          articles.append({
                              "title": BeautifulSoup(entry.title, "html.parser").get_text(),
                              "url": entry.link,
                              "source": feed_url.split("//")[1].split("/")[0],
                              "date": datetime.utcnow().isoformat() + "Z"
                          })
                      except Exception as e:
                          print(f"Error processing entry from {feed_url}: {str(e)}")
                          continue
                          
          with open("data/news.json", "w", encoding="utf-8") as f:
              json.dump({
                  "lastUpdated": datetime.utcnow().isoformat() + "Z",
                  "articles": articles[:150]
              }, f, ensure_ascii=False, indent=2)
          ' > scrape_news.py
          
          python scrape_news.py
      
      # Step 6: Commit changes
      - name: Commit and push
        run: |
          git config --global user.name "GitHub Actions"
          git config --global user.email "actions@github.com"
          git pull --rebase
          git add data/news.json
          git diff --quiet && git diff --staged --quiet || git commit -m "Auto-update news [skip ci]"
          git push
